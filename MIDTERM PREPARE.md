#MIDTERM PREPARE

- Be able to answer questions like the ones in Lesson Exercise Week1, Exercises 11 to 17
> normal distribution and student t distribution -> lesson week 1 page 7-9

- Be ready to calculate slope, intercept, SSR, SSE, SST, R^2, error degrees of freedom, model degrees of freedom, F-statistic with a spreadsheet, RMSE. The number of x, y samples could range anywhere between 8 to 20.
> **Slope** = ${\frac{Change\ in\ Y}{Change\ in\ X}}$ = ${\frac{y_1-y_0}{x_1-x_0}}$  
> **equation** $y = mx + b$  
> **intercept** = b when x is 0  
> **SSR** = $\Sigma_{i=1}^n(\hat{y}_i-\bar{y})^2$      
> **SSE** = $\Sigma_{i=1}^n({y_i}-\hat{y})^2$      
> **SST** = $\Sigma_{i=1}^n({y_i}-\bar{y})^2$      
> **R^2** =  ${\frac{SSR}{SST}} $ = ${\frac{SSR}{SSR+SSE}} $  
> **error degrees of freedom** =  **Error DF** = N - Model DF - 1(includes the intercept)  
> **model degrees of freedom** =  **Model DF** = Total Number of Variables   
> **F-statistic** =  F = ${\frac{\frac{MS_{model}}{MS_{error}}}{\frac{SSE}{Error DF}}}$  
> **RMSE** =  $\sqrt{\frac{\sum{(y actual - y {predicted})^2}}{number of samples}}$ = $\frac{\sqrt{\sum{(y-\hat{y})^2}}}{n} $  
- Be able to calculate R^2 and the F-Statistic by hand.
> see up
- Be able to build a linear regression model in Python for the least square regression model developed in your spreadsheet.
> Lesson Week2 page 14-15
- Be ready to manually calculate the p-value that appears in the model summary for the X coefficient in your Python model.
- Be able to manually create a model from the model summary that is generated by Python during multiple linear regression. Use only statistically significant coefficients. Interpret the model by describing the significant coefficients.
- Discuss the meaning behind the diagram in figure 6 of lesson week2.
> **ANOVA** to help determine the reliability of ther prediction model in terms of variance. High variance is undesirable.    
- Answer a short answer question on binning. Explain the benefits of binning and when it might be used.
> benefits: turn insignificant data into grouped data that become significant.   
> when the data is suppose to be significant but it didn't show the significant in the model summary.   
- Answer a short answer question on dummy variables. Explain their benefits and
why they are used.  
> Regression analysis treats all independent (X) variables in the analysis as
 numerical, Numerical variables are interval or ratio scale variables whose
 values are directly comparable  
> but some data is not meaningful in numerical, we have to convert these data   
> into categories to become meaning for in the model analysis.   
> Dummy variables are created in this situation to ‘trick’ the regression
> algorithm into correctly analyzing attribute variables.  The dummy variables
> act like ‘switches’ that turn various parameters on and off in an equation. 
- Explain the advantages and disadvantages of treating outliers.
> **adv**: we may get rid of unintentional(born year instead of age) or intentional error(not willing to give right age or right salary because of privace) data by treating outliers to get better
> analysis result without introducing bias.  
> **disadv**: the outliers are might be legitimate obsevation. It could be 
> hard to take outliers without introducing bias, and sometime it can make be
> hard to decide the rule for determining outliers.
>  
- Explain the concerns and benefits of imputing. Suggest alternatives for binning and why they might be used.
> Imputing may bring positive outcome but not always, 
Logistic regression:

- Know what a sigmoid function is used for and what it looks like. Be able to draw it.
- Be able to manually calculate a confusion matrix.
- Be able to manually calculate and interpret accuracy, precision, and recall scores.
- I may add 2 or 3 topics for logistic regression next week.

